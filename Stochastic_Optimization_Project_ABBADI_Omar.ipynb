{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "For this project, I implemented stochastic gradient and adagrad on the multinomial logistic regression with squared 2-norm regularization and calculated the accuracy.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0xMLpuW2XZAg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "Y9bk_-PQYmEt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random"
      ],
      "metadata": {
        "id": "VS7mW7Md-_iu"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "QyXUdl6o09IO"
      },
      "outputs": [],
      "source": [
        "# Load the MNIST dataset\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train_full, y_train_full), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalize the data: scale the pixel values from 0-255 to 0-1\n",
        "x_train_full, x_test = x_train_full / 255.0, x_test / 255.0\n",
        "\n",
        "# Split the full training set into a smaller training set and a validation set\n",
        "# For example, use 50,000 images for training and 10,000 for validation\n",
        "x_train, x_valid = x_train_full[:50000], x_train_full[50000:]\n",
        "y_train, y_valid = y_train_full[:50000], y_train_full[50000:]\n",
        "\n",
        "# Now, x_train and y_train are the training set,\n",
        "# x_valid and y_valid are the validation set,\n",
        "# and x_test and y_test are the test set."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d = np.shape(x_train[1])        # The dimension of images\n",
        "n = 10                          # Number of digits\n",
        "N = len(x_train)                # Number of images in the traning dataset\n",
        "Nv = len(x_valid)               # Number of images in the validation set\n",
        "K = 30000                       # Number of iterations for the algorithm"
      ],
      "metadata": {
        "id": "1ZlMwER56yi3"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert labels to one-hot encoding\n",
        "y_train_one_hot = tf.keras.utils.to_categorical(y_train, num_classes=10)\n",
        "y_valid_one_hot = tf.keras.utils.to_categorical(y_valid, num_classes=10)"
      ],
      "metadata": {
        "id": "DUI7Fejr72mh"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculating the gradient"
      ],
      "metadata": {
        "id": "foB6-D5lYesC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\forall i \\in \\{1, \\dots, n\\}$ The gradient of $F_i$ with respect to $w_{t,p} ~ \\forall t \\in \\{1, \\dots, d\\} ~ \\& ~ \\forall p \\in \\{0, \\dots, 9\\}$ is $$\\nabla_{w_{t, p}} F_i = x_{i,t}\\frac{\\exp~(\\sum_{k = 1}^{d} x_{i,k}w_{k,p} + w_{0,p})}{\\sum_{j = 0}^{9} \\exp~(\\sum_{k = 1}^{d} x_{i,k}w_{k,j} + w_{0,j})} - y_{i,p}x_{i,t} + w_{t, p}$$ The gradient of $F_i$ respect to $w_{0,p} ~ \\forall p \\in \\{0, \\dots, 9\\}$ is $$\\nabla_{w_{0,p}} F_i = \\frac{\\exp~(\\sum_{k = 1}^{d} x_{i,k}w_{k,p} + w_{0,p})}{\\sum_{j = 0}^{9} \\exp~(\\sum_{k = 1}^{d} x_{i,k}w_{k,j} + w_{0,j})} - y_{i,p}$$\n"
      ],
      "metadata": {
        "id": "u6GNKGjzS8_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient(m, W, W0, alpha, X, Y):\n",
        "    # m is the index of the image in X\n",
        "    # W is the weight matrix of dimensions (d, n)\n",
        "    # W0 is the bias vector of size n\n",
        "    # alpha is the regularization parameter\n",
        "    # X is the input data matrix\n",
        "    # Y is the label matrix in one-hot encoding format\n",
        "\n",
        "    G_W = np.zeros_like(W)  # Gradient with respect to W\n",
        "    G_W0 = np.zeros_like(W0)  # Gradient with respect to W0\n",
        "\n",
        "    # Compute gradients\n",
        "    A = np.zeros(n)\n",
        "    B = 0\n",
        "    for j in range(n):\n",
        "        A[j] = np.exp(np.vdot(X[m], W[:, :, j]) + W0[j])\n",
        "        B += A[j]\n",
        "    for j in range(n):\n",
        "        G_W[:, :, j] = X[m] * (A[j] / B) - Y[m, j] * X[m] + alpha * W[:, :, j]\n",
        "        G_W0[j] = A[j] / B - Y[m, j]\n",
        "\n",
        "    return G_W, G_W0"
      ],
      "metadata": {
        "id": "rSRU1Gda6E7t"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stochastic Gradient"
      ],
      "metadata": {
        "id": "Gx916rmPYUvr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stochastic_Gradient(alpha, W, W0, gamma, K, f, X, Y):\n",
        "    for k in range(K):\n",
        "        m = random.randint(0, N-1)         # Random index\n",
        "        G = f(m, W, W0, alpha, X, Y)       # The gradient with respect to both W and W0\n",
        "        W = W - gamma*G[0]                 # The W Update\n",
        "        W0 = W0 - gamma*G[1]               # The W0 Update\n",
        "    return W"
      ],
      "metadata": {
        "id": "ZDoMqc3U8NGk"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W = stochastic_Gradient(0.7, np.zeros((d[0], d[1], n)), np.zeros(n), 1/np.sqrt(K), K, gradient, x_train, y_train_one_hot)"
      ],
      "metadata": {
        "id": "7DywuMgu8VQC"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Accuracy(W):\n",
        "    Nt = N - Nv\n",
        "    S = 0\n",
        "    for i in range(Nt, N):\n",
        "        argmax = np.argmax([(np.vdot(x_valid[i - Nt], W[:,:,j])) for j in range(n)]) # Find the index with the highest dot product\n",
        "        if argmax == y_valid[i - Nt]:  # Check if the predicted label matches the actual label\n",
        "            S += 1\n",
        "    return S / Nv  # Calculate and return the accuracy"
      ],
      "metadata": {
        "id": "1I7zDA7nJODE"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Accuracy(W)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iy2MbEdKFMzH",
        "outputId": "7a4aadb3-e788-402b-a388-afd201c2e1b7"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6444"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def find_best_alpha(alphas):\n",
        "    best_alpha = None\n",
        "    highest_accuracy = 0\n",
        "    for alpha in alphas:\n",
        "        W = stochastic_Gradient(alpha, np.zeros((d[0], d[1], n)), np.zeros(n), 1e-3/np.sqrt(K), K, gradient, x_train, y_train_one_hot)\n",
        "        # Evaluate accuracy with the trained model\n",
        "        current_accuracy = Accuracy(W)\n",
        "        # Update best alpha if the current model performs better\n",
        "        if current_accuracy > highest_accuracy:\n",
        "            best_alpha = alpha\n",
        "            highest_accuracy = current_accuracy\n",
        "    return best_alpha, highest_accuracy"
      ],
      "metadata": {
        "id": "5CVQTDK7MGw5"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "find_best_alpha(np.linspace(0.001, 3, 10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKTovKerMks-",
        "outputId": "62408e70-205a-4ea1-8d73-1c4b90f76449"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.33422222222222225, 0.7478)"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adagrad"
      ],
      "metadata": {
        "id": "nqYp3vROYalI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def adagrad(f, W, W0, alpha, K, X, Y, epsilon=1e-8):\n",
        "    # Initialize gradient accumulation vectors for W and W0\n",
        "    G_W = np.zeros(W.shape)\n",
        "    G_W0 = 0\n",
        "\n",
        "    for k in range(K):\n",
        "        m = random.randint(0, N-1)    # Random index\n",
        "        G = f(m, W, W0, alpha, X, Y)  # The gradient with respect to both W and W0\n",
        "\n",
        "        # Accumulate squared gradients\n",
        "        G_W += G[0]**2\n",
        "        G_W0 += G[1]**2\n",
        "\n",
        "        # Update W and W0 with adaptive learning rates\n",
        "        W = W - (alpha / (np.sqrt(G_W) + epsilon)) * G[0]\n",
        "        W0 = W0 - (alpha / (np.sqrt(G_W0) + epsilon)) * G[1]\n",
        "\n",
        "    return W"
      ],
      "metadata": {
        "id": "ws5FPvoXWER7"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W = adagrad(gradient, np.zeros((d[0], d[1], n)), np.zeros(n), 0.7, K, x_train, y_train_one_hot)"
      ],
      "metadata": {
        "id": "kvWGdjErWX1c"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Accuracy(W)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ej81CHP9XKrf",
        "outputId": "324a289c-5a27-42de-be18-d79560f1807d"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5491"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def find_best_alpha_ada(alphas):\n",
        "    best_alpha = None\n",
        "    highest_accuracy = 0\n",
        "    for alpha in alphas:\n",
        "        W = adagrad(gradient, np.zeros((d[0], d[1], n)), np.zeros(n), alpha, K, x_train, y_train_one_hot)\n",
        "        # Evaluate accuracy with the trained model\n",
        "        current_accuracy = Accuracy(W)\n",
        "        # Update best alpha if the current model performs better\n",
        "        if current_accuracy > highest_accuracy:\n",
        "            best_alpha = alpha\n",
        "            highest_accuracy = current_accuracy\n",
        "    return best_alpha, highest_accuracy"
      ],
      "metadata": {
        "id": "a4l_FXFAWsSO"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "find_best_alpha_ada(np.linspace(0.001, 3, 10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4I4TpW7XTuK",
        "outputId": "a19e0917-5c6d-4e7a-cea7-3e47ddfebaed"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.001, 0.8501)"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    }
  ]
}